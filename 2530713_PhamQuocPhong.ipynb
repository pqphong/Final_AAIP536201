{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-17T15:55:33.615633Z",
     "iopub.status.busy": "2025-08-17T15:55:33.615058Z",
     "iopub.status.idle": "2025-08-17T15:55:37.028833Z",
     "shell.execute_reply": "2025-08-17T15:55:37.028030Z",
     "shell.execute_reply.started": "2025-08-17T15:55:33.615607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install ultralytics -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "data = \"dataset/data.yaml\"\n",
    "epochs = 1000\n",
    "device = 0\n",
    "batch = 32\n",
    "project = \"miotcd\"\n",
    "exist_ok = False\n",
    "resume = True\n",
    "\n",
    "class YOLOv12:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = YOLO(model_path)\n",
    "\n",
    "    def train(self, data, epochs, device, batch, project, exist_ok,resume):\n",
    "        return self.model.train(\n",
    "            data=data,\n",
    "            epochs=epochs,\n",
    "            device=device,\n",
    "            batch=batch,\n",
    "            project=project,\n",
    "            exist_ok=exist_ok,\n",
    "            resume=resume\n",
    "        )\n",
    "\n",
    "model_path = \"last.pt\"\n",
    "detector = YOLOv12(model_path)\n",
    "\n",
    "results = detector.train(\n",
    "    data=data,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    "    batch=batch,\n",
    "    project=project,\n",
    "    exist_ok=exist_ok,\n",
    "    resume=resume\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T16:39:07.948745Z",
     "iopub.status.busy": "2025-08-17T16:39:07.948433Z",
     "iopub.status.idle": "2025-08-17T17:10:02.036247Z",
     "shell.execute_reply": "2025-08-17T17:10:02.035481Z",
     "shell.execute_reply.started": "2025-08-17T16:39:07.948722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "class Detector:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = YOLO(model_path)\n",
    "\n",
    "    def detect(self, src_img):\n",
    "        results = self.model.predict(\n",
    "            source=src_img,\n",
    "        )\n",
    "        bboxes = results[0].boxes.xywh.cpu().numpy()\n",
    "        bboxes[:,:2] = bboxes[:,:2] - bboxes[:,2:]/2\n",
    "        scores = results[0].boxes.conf.cpu().numpy()\n",
    "        class_ids = results[0].boxes.cls.cpu().numpy()\n",
    "        return bboxes, scores, class_ids\n",
    "    \n",
    "model_path = \"miotcd/train/weights/best.pt\"\n",
    "src_img = \"dataset/images/test/00000035.jpg\"\n",
    "detector = Detector(model_path)\n",
    "results = detector.detect(src_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation yolo V12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.202 üöÄ Python-3.13.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7806MiB)\n",
      "Model summary (fused): 92 layers, 25,846,129 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 72.2¬±52.0 MB/s, size: 25.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /media/quocphong/01DC253D40D5ABD0/DATA/00_MASTER_COURSE/AAIP536201_TTUDTTNT/Finnal_prj/YOLOv12/dataset/labels/val... 11000 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11000/11000 4.3Kit/s 2.6s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /media/quocphong/01DC253D40D5ABD0/DATA/00_MASTER_COURSE/AAIP536201_TTUDTTNT/Finnal_prj/YOLOv12/dataset/labels/val.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 688/688 5.9it/s 1:56<0.2ss\n",
      "                   all      11000      35070      0.796      0.796      0.835      0.641\n",
      "     articulated_truck        799        890      0.846      0.915      0.931      0.799\n",
      "               bicycle        181        246       0.87      0.919      0.936      0.674\n",
      "                   bus        892       1027      0.928      0.976      0.984      0.918\n",
      "                   car       9090      23426      0.904      0.899      0.958      0.735\n",
      "            motorcycle        158        175       0.84       0.92      0.925      0.643\n",
      "     motorized_vehicle       2000       2519      0.644      0.366      0.508       0.32\n",
      " non-motorized_vehicle        194        197      0.595      0.701       0.68      0.494\n",
      "            pedestrian        374        715      0.809      0.631       0.74      0.404\n",
      "          pickup_truck       3172       4435        0.9      0.898      0.941      0.787\n",
      "     single_unit_truck        541        573      0.741      0.724      0.764      0.594\n",
      "              work_van        811        867      0.678      0.804      0.823      0.683\n",
      "Speed: 0.1ms preprocess, 9.4ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1m/media/quocphong/01DC253D40D5ABD0/DATA/00_MASTER_COURSE/AAIP536201_TTUDTTNT/Finnal_prj/YOLOv12/runs/detect/val12\u001b[0m\n",
      "0.6408034475728911\n",
      "0.8354076419227122\n",
      "0.7183956552468476\n",
      "[    0.79852     0.67389     0.91769     0.73462     0.64282     0.31953     0.49352     0.40396     0.78743     0.59355     0.68331]\n",
      "[{\"Predicted\":\"articulated_truck\",\"articulated_truck\":812.0,\"bicycle\":0.0,\"bus\":1.0,\"car\":1.0,\"motorcycle\":0.0,\"motorized_vehicle\":51.0,\"non_motorized_vehicle\":6.0,\"pedestrian\":0.0,\"pickup_truck\":0.0,\"single_unit_truck\":45.0,\"work_van\":1.0,\"background\":104.0},{\"Predicted\":\"bicycle\",\"articulated_truck\":0.0,\"bicycle\":225.0,\"bus\":0.0,\"car\":0.0,\"motorcycle\":2.0,\"motorized_vehicle\":0.0,\"non_motorized_vehicle\":0.0,\"pedestrian\":18.0,\"pickup_truck\":0.0,\"single_unit_truck\":0.0,\"work_van\":0.0,\"background\":28.0},{\"Predicted\":\"bus\",\"articulated_truck\":4.0,\"bicycle\":0.0,\"bus\":997.0,\"car\":7.0,\"motorcycle\":0.0,\"motorized_vehicle\":18.0,\"non_motorized_vehicle\":3.0,\"pedestrian\":0.0,\"pickup_truck\":1.0,\"single_unit_truck\":12.0,\"work_van\":8.0,\"background\":49.0},{\"Predicted\":\"car\",\"articulated_truck\":1.0,\"bicycle\":0.0,\"bus\":0.0,\"car\":21598.0,\"motorcycle\":1.0,\"motorized_vehicle\":546.0,\"non_motorized_vehicle\":4.0,\"pedestrian\":0.0,\"pickup_truck\":287.0,\"single_unit_truck\":0.0,\"work_van\":144.0,\"background\":2840.0},{\"Predicted\":\"motorcycle\",\"articulated_truck\":0.0,\"bicycle\":3.0,\"bus\":0.0,\"car\":1.0,\"motorcycle\":163.0,\"motorized_vehicle\":1.0,\"non_motorized_vehicle\":0.0,\"pedestrian\":2.0,\"pickup_truck\":0.0,\"single_unit_truck\":1.0,\"work_van\":0.0,\"background\":24.0},{\"Predicted\":\"motorized_vehicle\",\"articulated_truck\":15.0,\"bicycle\":0.0,\"bus\":6.0,\"car\":289.0,\"motorcycle\":0.0,\"motorized_vehicle\":1058.0,\"non_motorized_vehicle\":13.0,\"pedestrian\":0.0,\"pickup_truck\":48.0,\"single_unit_truck\":35.0,\"work_van\":6.0,\"background\":795.0},{\"Predicted\":\"non_motorized_vehicle\",\"articulated_truck\":8.0,\"bicycle\":0.0,\"bus\":2.0,\"car\":4.0,\"motorcycle\":1.0,\"motorized_vehicle\":47.0,\"non_motorized_vehicle\":132.0,\"pedestrian\":0.0,\"pickup_truck\":0.0,\"single_unit_truck\":6.0,\"work_van\":0.0,\"background\":66.0},{\"Predicted\":\"pedestrian\",\"articulated_truck\":0.0,\"bicycle\":10.0,\"bus\":0.0,\"car\":2.0,\"motorcycle\":2.0,\"motorized_vehicle\":2.0,\"non_motorized_vehicle\":0.0,\"pedestrian\":514.0,\"pickup_truck\":0.0,\"single_unit_truck\":0.0,\"work_van\":0.0,\"background\":155.0},{\"Predicted\":\"pickup_truck\",\"articulated_truck\":0.0,\"bicycle\":0.0,\"bus\":0.0,\"car\":313.0,\"motorcycle\":0.0,\"motorized_vehicle\":77.0,\"non_motorized_vehicle\":3.0,\"pedestrian\":0.0,\"pickup_truck\":4012.0,\"single_unit_truck\":18.0,\"work_van\":10.0,\"background\":347.0},{\"Predicted\":\"single_unit_truck\",\"articulated_truck\":25.0,\"bicycle\":0.0,\"bus\":7.0,\"car\":3.0,\"motorcycle\":0.0,\"motorized_vehicle\":72.0,\"non_motorized_vehicle\":9.0,\"pedestrian\":0.0,\"pickup_truck\":13.0,\"single_unit_truck\":402.0,\"work_van\":7.0,\"background\":117.0},{\"Predicted\":\"work_van\",\"articulated_truck\":0.0,\"bicycle\":0.0,\"bus\":2.0,\"car\":168.0,\"motorcycle\":0.0,\"motorized_vehicle\":22.0,\"non_motorized_vehicle\":3.0,\"pedestrian\":0.0,\"pickup_truck\":17.0,\"single_unit_truck\":16.0,\"work_van\":660.0,\"background\":233.0},{\"Predicted\":\"background\",\"articulated_truck\":25.0,\"bicycle\":8.0,\"bus\":12.0,\"car\":1040.0,\"motorcycle\":6.0,\"motorized_vehicle\":625.0,\"non_motorized_vehicle\":24.0,\"pedestrian\":181.0,\"pickup_truck\":57.0,\"single_unit_truck\":38.0,\"work_van\":31.0,\"background\":0.0}]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"ultralytics\")\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"best.pt\")  # load a custom model\n",
    "metrics = model.val(data=\"data.yaml\", plots=True)\n",
    "print(metrics.box.map)  # mAP50-95\n",
    "print(metrics.box.map50)  # mAP50\n",
    "print(metrics.box.map75)  # mAP75\n",
    "print(metrics.box.maps)  # list of mAP50-95 for each category\n",
    "print(metrics.confusion_matrix.to_json())\n",
    "\n",
    "# from ultralytics.utils.plotting import plot_results\n",
    "\n",
    "# # V√≠ d·ª• v·ªõi file results.csv trong train7\n",
    "# plot_results(\"miotcd_test/train/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ===== Configuration =====\n",
    "MODEL_PATH = \"oversam_best_49ep.pt\"      # Path to the trained YOLO model\n",
    "VIDEO_PATH = \"VideoTest/MVI_20011.avi\"   # Path to the input video\n",
    "OUTPUT_PATH = \"output_tracking_2.mp4\"    # Path to save the output video\n",
    "TRACKER_YAML = \"botsort.yaml\"            # Path to the tracker configuration file (e.g., botsort.yaml)\n",
    "\n",
    "MAX_TRAIL = 30       # Maximum number of points to store for each trail\n",
    "THICKNESS = 2        # Thickness of the trail line\n",
    "FONT_SIZE = 0.4      # Font size for annotations (smaller than default)\n",
    "BOX_THICKNESS = 1    # Bounding box border thickness (thinner)\n",
    "\n",
    "# ===== Helper Functions =====\n",
    "def color_for_id(tid: int):\n",
    "    \"\"\"Generates a consistent, pseudo-random color for a given track ID.\"\"\"\n",
    "    random.seed(tid + 12345)  # Use ID as seed for consistency\n",
    "    # Generate a bright, non-dark color (B, G, R)\n",
    "    return (random.randint(40, 255), random.randint(40, 255), random.randint(40, 255))\n",
    "\n",
    "# ===== Initialization =====\n",
    "model = YOLO(MODEL_PATH)\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "# Get original video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps_video = cap.get(cv2.CAP_PROP_FPS)\n",
    "# Calculate delay for cv2.waitKey() to match original video speed\n",
    "delay = int(1000 / fps_video) if fps_video and fps_video > 0 else 1\n",
    "\n",
    "# Create VideoWriter object to save the result\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec for MP4\n",
    "out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps_video, (width, height))\n",
    "\n",
    "# Dictionary to store trails.\n",
    "# key: track_id, value: deque (a list with a maximum length)\n",
    "trails = defaultdict(lambda: deque(maxlen=MAX_TRAIL))\n",
    "\n",
    "print(f\"üîç Processing video {VIDEO_PATH} ...\")\n",
    "print(f\"üíæ Results will be saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break  # End of video\n",
    "\n",
    "    start = time.time() # Start timer for performance measurement\n",
    "\n",
    "    # Run YOLO tracking on the frame\n",
    "    results = model.track(\n",
    "        source=frame,\n",
    "        tracker=TRACKER_YAML,  # Specify the tracker config\n",
    "        persist=True,          # Maintain track IDs between frames\n",
    "        conf=0.25,             # Confidence threshold\n",
    "        iou=0.45,              # IOU threshold for NMS\n",
    "        verbose=False,         # Disable detailed console output\n",
    "        show=False             # Do not display results automatically\n",
    "    )\n",
    "\n",
    "    r = results[0]  # Get results for the first (and only) image\n",
    "    \n",
    "    # Use the built-in .plot() method to draw boxes and labels\n",
    "    # We use custom thickness and font size\n",
    "    annotated = r.plot(line_width=BOX_THICKNESS, font_size=FONT_SIZE)\n",
    "\n",
    "    boxes = r.boxes\n",
    "    # Check if any objects were tracked in this frame\n",
    "    if boxes is not None and boxes.id is not None:\n",
    "        # Get track IDs and their corresponding bounding boxes\n",
    "        ids = boxes.id.int().cpu().tolist()\n",
    "        xyxy = boxes.xyxy.int().cpu().tolist()\n",
    "\n",
    "        # Update & draw trails\n",
    "        for (tid, (x1, y1, x2, y2)) in zip(ids, xyxy):\n",
    "            # Calculate the center point of the bounding box\n",
    "            cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)\n",
    "            # Add the new center point to this ID's trail\n",
    "            trails[tid].append((cx, cy))\n",
    "\n",
    "        # Draw the trails for all currently tracked objects\n",
    "        for tid, pts in trails.items():\n",
    "            if len(pts) > 1: # Need at least two points to draw a line\n",
    "                col = color_for_id(tid) # Get the color for this ID\n",
    "                # Draw line segments connecting all points in the trail\n",
    "                for i in range(1, len(pts)):\n",
    "                    cv2.line(annotated, pts[i-1], pts[i], col, THICKNESS)\n",
    "\n",
    "    # Calculate inference time\n",
    "    end = time.time()\n",
    "    inf_ms = (end - start) * 1000  # Inference time in milliseconds\n",
    "    fps = 1 / (end - start + 1e-8) # Frames Per Second\n",
    "\n",
    "    # Display FPS and inference time on the frame\n",
    "    text = f\"FPS: {fps:.2f} | {inf_ms:.2f} ms\"\n",
    "    cv2.putText(\n",
    "        annotated, text,\n",
    "        (annotated.shape[1] - 400, 30), # Position (top-right area)\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.6, (0, 255, 0), 2, cv2.LINE_AA\n",
    "    )\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated)\n",
    "\n",
    "    # Optional: Display the tracking results in a window\n",
    "    cv2.imshow(\"YOLO Tracking (Saving Output)\", annotated)\n",
    "    \n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(delay) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release all resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"‚úÖ Done! Video saved successfully.\")\n",
    "print(f\"üìÅ File: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8049930,
     "sourceId": 12735309,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8050133,
     "sourceId": 12735491,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8050598,
     "sourceId": 12736191,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 255590133,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "yolo12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
